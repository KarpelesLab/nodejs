package nodejs

import (
	"context"
	"runtime"
	"sync"
	"sync/atomic"
	"time"
)

type Pool struct {
	queue     []*Process
	factory   *Factory
	maxProcs  uint64
	procs     uint64
	spawner   atomic.Bool // is spawner running?
	addcond   *sync.Cond  // need add
	availcond *sync.Cond  // available (can take)
	lk        sync.Mutex
	Timeout   time.Duration // timeout for running nodejs, defaults to 10 second if zero
}

// NewPool returns a pool of nodejs processes generated by the factory. The pool will
// always generate at least queueSize processes in memory and will start new ones as
// they are taken. Passing queueSize <=0 will use the number of CPUs.
func (f *Factory) NewPool(queueSize, maxProcs int) *Pool {
	if queueSize <= 0 {
		queueSize = runtime.NumCPU()
	}
	if maxProcs <= 0 {
		maxProcs = runtime.NumCPU()
	}

	p := &Pool{
		maxProcs: uint64(maxProcs),
		factory:  f,
	}
	p.addcond = sync.NewCond(&p.lk)
	p.availcond = sync.NewCond(&p.lk)

	go p.run()

	return p
}

// TakeIfAvailable returns a [Process] if any is available, or nil if not
func (pool *Pool) TakeIfAvailable() *Process {
	pool.lk.Lock()
	defer pool.lk.Unlock()

	if len(pool.queue) == 0 {
		return nil
	}
	res := pool.queue[0]
	pool.queue = pool.queue[1:]
	return res
}

// Take returns a [Process] from the pool and will wait until one is available,
// unless the context is cancelled.
func (pool *Pool) Take(ctx context.Context) (*Process, error) {
	pool.lk.Lock()
	defer pool.lk.Unlock()

	def := context.AfterFunc(ctx, func() {
		pool.availcond.L.Lock()
		defer pool.availcond.L.Unlock()
		pool.availcond.Broadcast()
	})
	defer def()

	for {
		if e := ctx.Err(); e != nil {
			return nil, e
		}
		if len(pool.queue) > 0 {
			res := pool.queue[0]
			pool.queue = pool.queue[1:]
			return res, nil
		}
		pool.availcond.Wait()
	}
}

// TakeTimeout will return a [Process] taken from the pool if any is available
// before the timeout expires.
func (pool *Pool) TakeTimeout(t time.Duration) (*Process, error) {
	ctx, cancel := context.WithTimeout(context.Background(), t)
	defer cancel()

	return pool.Take(ctx)
}

func (pool *Pool) addProcess(p *Process) {
	pool.lk.Lock()
	defer pool.lk.Unlock()

	pool.queue = append(pool.queue, p)
	pool.availcond.Broadcast()
}

func (pool *Pool) run() {
	if !pool.spawner.CompareAndSwap(false, true) {
		// means run() is being called more than once
		return
	}
	defer pool.spawner.Store(false)

	for {
		if atomic.LoadUint64(&pool.procs) > pool.maxProcs {
			// too many running, stop
			// Note: we can have a very special edge case where we end at the exact time all processes stop
			// this is very unlikely to happen, especially if we have multiple processes
			return
		}

		timeout := pool.Timeout
		if timeout <= 0 {
			timeout = 10 * time.Second
		}
		newProc, err := pool.factory.NewWithTimeout(timeout)
		if err != nil {
			time.Sleep(10 * time.Second)
			continue
		}

		// ensure procs is updated if needed
		atomic.AddUint64(&pool.procs, 1)
		newProc.cleanup = append(newProc.cleanup, pool.procsSubOne)

		// add to queue
		pool.addProcess(newProc)
	}
}

func (pool *Pool) procsSubOne() {
	// sub 1 from pool.procs & wake up cond just in case
	atomic.AddUint64(&pool.procs, ^uint64(0))
	if !pool.spawner.Load() {
		go pool.run()
	}
}
